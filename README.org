* Project Literacy

#+html: <p align="center">Hand Writting Recognition Using TensorFlow</p>

** Background

   
   This project was made to solve a huge problem we have in our world. How do
   we digitilize the trillions of physical documents that we have archived? Some
   of these archives include important information such as crimminal records,
   scientific studies, patient records, family history, etc. But unless we hired
   the whole world we would still need several genrations to complete this task.

   But the solution to this is machine learning, but to be more precise it's
   deep learning. Deep learning is based of what we know about neurological
   processing. In the brain we have many networks of neurons that produce an
   "output" or result. These results are then processed by other networks and so
   on. This is deep learning a model. The model is the brain and the layers are
   the neural networks.

** Problem[ S ]
   
   1. Messy
   2. Huge data size 
   3. Availability of Data
   4. Splitting data.
   
   It is very difficult for a human to learn to read and it's the same for
   computers. Let's face the facts, us humans are very messy and our
   handwritting reflect that. We have thousands of ways to write the same
   character or number.
   
   In this we will focus on letters since processing numbers has been succesfuly
   accomplished. But even without numbers this leaves us a huge dataset that our
   machine must learn from. We need thousands of examples for each letter to be
   able to correctly learn.

   Sure there are millions of documents and handwritting examples in the world but 
   to make these usable it would take a whole team to do. 

   To be able to read documents we must be able to break words into individual letters
   and then process those.

** [[https://www.nist.gov/itl/iad/image-group/emnist-dataset][Dataset]]

   We will be using the EMNIST dataset which is a subset of the MNIST dataset. Spcifically we 
   will use the emnist-letters.

#+html: <p align="center"><img src="img/graph_one.png" /></p>

   As we can see we have 26 classes and about 6,000 samples for each. The data will be respresented
   in a MATLab format.

** Tools

*** [[https://www.python.org/downloads/release/python-350/][Python 3.5]]


*** [[ https://www.tensorflow.org/][TensorFlow]]


    Google's computational library that was built for machine learning and deep learning.
    
    *Install*

    #+BEGIN_SRC 
    pip install tensorflow
    #+END_SRC
   
    
*** [[https://keras.io/#installation][Keras]]

    #+BEGIN_QUOTE
    Keras is a high-level neural networks API, written in Python and capable of
    running on top of TensorFlow, CNTK, or Theano. It was developed with a focus
    on enabling fast experimentation. *Being able to go from idea to result with
    the least possible delay is key to doing good research.*
    #+END_QUOTE

    #+BEGIN_SRC 
    pip install keras
    #+END_SRC
   
   

*** [[http://www.numpy.org/][Numpy]]

    #+BEGIN_QUOTE
    NumPy is the fundamental package for scientific computing with Python. It
    contains among other things:

    - a powerful N-dimensional array object sophisticated (broadcasting)
    functions
    - tools for integrating C/C++ and Fortran code
    - useful linear algebra, Fourier transform, and random number capabilities 
    
    Besides its obvious scientific uses, NumPy can also be used as an efficient
    multi-dimensional container of generic data. Arbitrary data-types can be
    defined. This allows NumPy to seamlessly and speedily integrate with a wide
    variety of databases.
    #+END_QUOTE

** Type of Learning [ C.N.N. ]

   We will be using [[http://deeplearning.net/tutorial/lenet.html][convolution neural networks]] in this type of machine
   learning. As the link article mentions deep learning is based on models.
   Various layers using each others output to come to a result.

*** Layer
    
    Here 
    - Conv2D x 2
      
      This is our 2 dimensional neural network. we specify that it will compute
      5x5 for 32 features.

    - LeakyRelU
      
      From basic machine learning we learned about the sigmoid function, an
      activation function.
      #+html: <p align="center"><img src="img/sig.svg"/></p>
      
      RelU or Rectified Liniear unit acts as our activation function for our
      CNN. We will not go into the LeakyRelU specification in this paper to
      avoid getting into complex math formulas. Just know that this is an
      aproximation of the Rectifier function.

      #+html: <p align="center"><img src="img/relu.svg"/></p>

   
** Model 

   #+html: <p align="center"><img src="img/conv_model.png"/></p>

** finally the code
   
   Here we load the data into a variable and then extract from it using the 
   io matlab interface. This example shows how the data for the train set is retrieved.

   #+BEGIN_SRC python
    data = io.loadmat("./emnist-letters.mat")
    train = data["dataset"][0][0][0][0][0][0].astype(np.float32)
    train_labels = data["dataset"][0][0][0][0][0][1]
   #+END_SRC
   


   Using the keras api we will do data augmentation. Data augmentation is meant
   to help us out with the low number of examples that are given to us by the
   EMNIST-Letters set. We will give it certain parameters that will either
   rotate, flip, shear, zoom, and shift our pictures. This has proven to be
   worth while.


*** WARNING _DO NOT ATTEMPT IF RUNNING ON SLOWER SYSTEM_

   Recommended:

   32 GB of RAM
   NVIDIA machine learning graphics card

   Also if you're using a laptop, try to keep room well ventilated as laptop can become extremly hot.
   
   #+BEGIN_QUOTE
   "Has someone been using the oven in here?" - unaware roommate
   #+END_QUOTE
   
   _*This may take up to 16-24 hours to run completely*_

   Why does this take so long?

   Well if you read above, we are using ensembles to learn. This means that we are training
   various models and then comparing to get the best result. To train these models correctly
   we need to run epochs on every single one, and every epoch is expected to take 10-20 minutes.
   With TensorFlow's model this could take a long time.

   Now a bit of an explanation to this code. It  will use our augmented data to learn and  then 
   make predictions.
   
   Behind that we will be collecting our weights for previous runs to be able to see what worked best and
   to have a better starting point to future learning.

   #+BEGIN_SRC python

   for it in range(num_it):
    epoch = (it + 1)* total + weights_epoch
    print('\ncurrent iteration: ' + str(it) + '\ncurrent epoch: ' + str(epoch) +'\n')

    for index,cur_model in enumerate(models):
        print('I am in the second loop')
        cur_model.model.optimizer.lr = 0.000001

        trained = cur_model.model.fit_generator(all_batches,steps_per_epoch=steps, epochs=total, verbose=0,
                            validation_data=testing, validation_steps=val_steps)
        cur_model.model.save_weights("learning/weights/{:03d}epochs_weights_model_{}.pkl".format(epoch, index))
    all_preds = np.stack([m.model.predict(test, batch_size=eval_batch_size) for m in models])
    avg_preds = all_preds.mean(axis=0)
    test_error = (1 - keras.metrics.categorical_accuracy(test_labels, avg_preds).eval().mean()) * 100

    with open("learning/history/test_errors_epoch_{:03d}.txt".format(epoch), "w") as text_file:
        print('i am in the third loop')
        text_file.write("epoch: {} test error on ensemble: {}\n".format(epoch, test_error))
        for m in models:
            pred = np.array(m.predict(test, batch_size=eval_batch_size))
            test_err = (1 - keras.metrics.categorical_accuracy(test_labels, pred).eval().mean()) * 100
            text_file.write("{}\n".format(test_err))

   #+END_SRC
* Some results

** 4 epochs

   #+html: <p align="center"><img src="img/4_epochs_2_hours.png"/></p>

** 16 epochs

   #+html: <p align="center"><img src="img/16_epochs_6_hours.png"/></p>
  

* What I learned 


** Data Collection

   As I began this project I spent over 4 weeks trying to get to get the right
   data. I was lead to the NIST data set and this basically gave me about 2 gigs
   worth of pictures divided into classes. Not knowing any better I tried
   tackling this huge monster. I did everything with these pictures. I re-sized,
   converted, and processed these 1.4 million pictures. All to find out that I
   didn't do it exactly how the api required it. If you'd like to see some of
   these failed attempt files please send me an email.


** Re-inventing the wheel

   While I tried to do as the tensorflow page asked this process while
   understandable was a waste of time. It was confusing and it focused more on
   the technicalities. But when I found Keras it made things a lot easier and it
   even shrunk the size of my code. Giving me more time to model building and
   gathering data.

** Learning takes time

   I learned that even with a fancy computer deep learning/convolutional
   learning just takes time. Since there are many layers to train and many gears
   to ccrank this is just part of. And it fixed my incorrect understanding that
   tensorflow would just make things faster and better. But it did make it
   better with the original EMNIST paper we learn that back then they could only
   get a 50% accuracy. But while doing this I got up to 95 percent. And that
   took a while, 16 hours to exact.
   
   
